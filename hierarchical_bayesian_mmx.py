# -*- coding: utf-8 -*-
"""Hierarchical Bayesian Log MMX.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UzdpGNnfJPDhpqLCuGqjvTP5oAC7VvAa

# **Import Requirements**
"""

# First delete runtime then run this before running anything below
import os
os.environ['MKL_THREADING_LAYER'] = 'GNU'

#  Import Required Libraries
!pip install pymc arviz aesara scipy

import numpy as np
import pandas as pd
import aesara.tensor as at
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import matplotlib.pyplot as plt
import pymc as pm
import arviz as az
import pickle

import pytensor.tensor as tt  # Import pytensor.tensor as tt

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('data.csv')
print("Data loaded, first 5 rows:")
df.head()

"""# **Process Training Data File**"""

df_original = df.copy()  # Create a copy of the original DataFrame

"""
The entire model was executed and variables which were insignificant have been removed.

Columns to be removed as they are not significant
me_cols
st_cols
mrkdn_cols
hldy_cols

"""

# Control and media variables
#me_cols = [col for col in df.columns if 'me_' in col] # Macro-economic variables (continuous)
#st_cols = ['st_ct'] # Store count (continuous)
#mrkdn_cols = [col for col in df.columns if 'mrkdn_' in col] # Markdown/Discount (continuous)
#hldy_cols = [col for col in df.columns if 'hldy_' in col] # Holidays (binary)
#seas_cols = [col for col in df.columns if 'seas_' in col] # Seasonality (binary)
seas_cols = ['seas_week_45', 'seas_week_46', 'seas_week_47']

#control_vars_continuous = me_cols + st_cols + mrkdn_cols  # Continuous control variables
control_vars_binary = seas_cols  # Binary control variables
control_vars = control_vars_binary  # All control variables

media_cols = [col for col in df.columns if 'mdsp_' in col]  # Media spending columns

sales_cols = ['sales']

# Step 1: Log-transform the media and sales data (log(1+x) to avoid issues with zeros)
df[media_cols] = np.log(df[media_cols] + 1)
#df[control_vars_continuous] = np.log(df[control_vars_continuous] + 1)
df[control_vars_binary] = df[control_vars_binary]  # No need to log-transform binary variables
y_sales = np.log(df['sales'] + 1).values  # Log-transform sales

# Step 2: Scale the media and control variables
media_scaler = MinMaxScaler()
#continuous_scaler = MinMaxScaler()

df[media_cols] = media_scaler.fit_transform(df[media_cols])
#df[control_vars_continuous] = continuous_scaler.fit_transform(df[control_vars_continuous])

# Binary variables remain unchanged (no scaling needed)
#X_controls_continuous = df[control_vars_continuous].values
X_controls_binary = df[control_vars_binary].values

# Combine continuous and binary control variables for the final matrix
X_controls = X_controls_binary
#X_controls = np.hstack((X_controls_continuous, X_controls_binary))

X_media = df[media_cols].values

"""# **Data Model Part 1**"""

# Apply adstock transformation to media data
def adstock_transform(x, alpha):
    adstocked = np.zeros_like(x)
    adstocked[0] = x[0]
    for t in range(1, len(x)):
        adstocked[t] = x[t] + alpha * adstocked[t-1]
    return adstocked

import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Adstock transformation function
def adstock_transform(x, alpha):
    adstocked = np.zeros_like(x)
    adstocked[0] = x[0]
    for t in range(1, len(x)):
        adstocked[t] = x[t] + alpha * adstocked[t-1]
    return adstocked

# Function to find optimal alpha for each media channel individually
def find_optimal_alpha_per_channel(X_media, y_sales, alpha_range=np.arange(0.2, 0.9, 0.1)):
    best_alphas = []
    mse_errors = []

    # Loop over each media channel
    for i, media_channel in enumerate(X_media.T):
        best_alpha = None
        best_mse = float('inf')
        channel_errors = []  # Store errors for each alpha

        for alpha in alpha_range:
            # Apply adstock transformation to this channel
            X_media_adstocked = adstock_transform(media_channel, alpha)

            # Fit linear regression model
            model = LinearRegression()
            model.fit(X_media_adstocked.reshape(-1, 1), y_sales)

            # Predict sales and calculate mean squared error
            y_pred = model.predict(X_media_adstocked.reshape(-1, 1))
            mse = mean_squared_error(y_sales, y_pred)
            mse = round(mse, 4)
            channel_errors.append(mse)

            # Keep track of the best alpha
            if mse < best_mse:
                best_mse = round(mse, 2)
                best_alpha = round(alpha, 1)

        best_alphas.append(best_alpha)
        mse_errors.append(channel_errors)

    return best_alphas, mse_errors



# Step 6: Apply the find_optimal_alpha_per_channel function
best_alphas, mse_errors = find_optimal_alpha_per_channel(X_media, y_sales)

# Debug: Print the best alphas and MSE errors
print(f"Best alphas for each channel: {best_alphas}")

# Assuming mse_errors were computed using alpha_range = np.arange(0.2, 0.9, 0.1)
alpha_range_short = np.arange(0.2, 0.9, 0.1)  # Shortened to match MSE values

# Plotting MSE vs Alpha for each media channel (adjusted)
for fig_idx in range(n_figures):
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 10))  # Create a grid of subplots
    axes = axes.ravel()  # Flatten axes array for easy iteration

    for idx in range(n_cols * n_rows):
        channel_idx = fig_idx * n_cols * n_rows + idx  # Calculate the current channel index
        if channel_idx < len(mdsp_cols):  # Ensure we don't exceed the number of media channels
            media_col = mdsp_cols[channel_idx]  # Get the media channel name

            # Plot MSE vs Alpha for the current media channel
            axes[idx].plot(alpha_range_short, mse_errors[channel_idx], marker='o', label=f'{media_col}')
            axes[idx].set_title(f'Alpha vs MSE for {media_col}')
            axes[idx].set_xlabel('Alpha')
            axes[idx].set_ylabel('MSE')
            axes[idx].grid(True)
            axes[idx].legend(loc='best')
        else:
            axes[idx].axis('off')

    plt.tight_layout()
    plt.show()

# Step 6: Apply the adstock transformation using the saved best alphas. This will calculate the media spend impact across weeks.
X_media_adstocked = np.array([adstock_transform(X_media[:, i], best_alphas[i]) for i in range(X_media.shape[1])]).T

X_media[0] # This is the non-adstocked value

X_media_adstocked[0] # This are the adstocked values for all channels for each week

"""# **Data Model - Part 2**"""

# This is to understand Diminishing returns

def hill_transform(x, ec, slope):
    safe_ec = tt.clip(ec, 1e-3, np.inf)
    safe_x = tt.clip(x, 1e-3, np.inf)
    return 1 / (1 + (safe_x / safe_ec)**(-slope))

# Define PyMC Model
with pm.Model() as bayesian_model:
    # Shared variables for media data (ensure X_media_adstocked is a 2D array)
    X_media_shared = pm.Data('X_media_shared', X_media_adstocked)

    # Priors for media coefficients (beta), ec, and slope
    beta_media = pm.Normal('beta_media', mu=1.0, sigma=0.1, shape=len(media_cols))
    ec = pm.Normal('ec', mu=1.0, sigma=0.5, shape=len(media_cols))
    slope = pm.Normal('slope', mu=1.0, sigma=0.5, shape=len(media_cols))

    media_effects = []
    for i in range(len(media_cols)):
        media_slice = X_media_adstocked[:, i]
        media_effect = beta_media[i] * hill_transform(media_slice, ec[i], slope[i])
        media_effects.append(media_effect)

    # Sum media effects
    media_sum = pm.math.sum(pm.math.stack(media_effects), axis=0)
    # Priors for control variable coefficients
    control_coefficients = pm.Normal('control_coefficients', mu=0, sigma=1, shape=len(control_vars))
    # Control effects
    control_effects = pm.math.dot(df[control_vars].values, control_coefficients)
    # Predicted log sales (base impact + media + control)
    intercept = pm.Normal('intercept', mu=10.8, sigma=1)  # Prior for the intercept
    mu = pm.Deterministic('mu', intercept + media_sum + control_effects)

    # Likelihood
    sigma = pm.HalfNormal('sigma', sigma=1)
    sales_observed = pm.Normal('sales_observed', mu=mu, sigma=sigma, observed=y_sales)

    # Sampling
    trace = pm.sample(1000, tune=500, return_inferencedata=True)

# Step 10: Summarize and inspect the trace
summary = az.summary(trace, hdi_prob=0.95)
print(summary)

"""
Download the parameters file to look at variables which are significant or insignificant

"""

import pandas as pd

# Assuming your `control_vars` and `media_cols` lists are already defined.
# Here is an example of how to map the indexes to their actual names.

# Assuming you have the summary DataFrame from PyMC3/ArviZ
summary_df = az.summary(trace, hdi_prob=0.95)  # Use the summary from your PyMC trace

# Map control_coefficients[i] to actual control variable names
for i, control_var in enumerate(control_vars):
    old_name = f'control_coefficients[{i}]'
    if old_name in summary_df.index:
        summary_df.rename(index={old_name: control_var}, inplace=True)

# Map beta_media[i] to actual media column names
for i, media_col in enumerate(media_cols):
    old_name = f'beta_media[{i}]'
    if old_name in summary_df.index:
        summary_df.rename(index={old_name: media_col}, inplace=True)

# Map slope[i] and ec[i] similarly if you have those in the summary
for i, media_col in enumerate(media_cols):
    old_name_slope = f'slope[{i}]'
    old_name_ec = f'ec[{i}]'
    if old_name_slope in summary_df.index:
        summary_df.rename(index={old_name_slope: f'slope_{media_col}'}, inplace=True)
    if old_name_ec in summary_df.index:
        summary_df.rename(index={old_name_ec: f'ec_{media_col}'}, inplace=True)


print(summary_df)

summary_df.to_csv('pymc_model_summary_with_names.csv', index=True)

print("Summary with actual variable names exported to 'pymc_model_summary_with_names.csv'.")

"""
Download the parameters file to look at variables which are significant or insignificant

"""

# Step 1: Extract posterior means for all key parameters
beta_media_mean = trace.posterior['beta_media'].mean(dim=['chain', 'draw']).values
ec_mean = trace.posterior['ec'].mean(dim=['chain', 'draw']).values
slope_mean = trace.posterior['slope'].mean(dim=['chain', 'draw']).values
control_coefficients_mean = trace.posterior['control_coefficients'].mean(dim=['chain', 'draw']).values
intercept_mean = trace.posterior['intercept'].mean(dim=['chain', 'draw']).values

# Step 2: Print extracted values for each media channel
n_channels = len(mdsp_cols)

print("\n--- Posterior Mean Parameters for Media Channels ---\n")
for i in range(n_channels):
    print(f"Channel {i+1}:")
    print(f"  Beta Media: {beta_media_mean[i]}")
    print(f"  EC (Effective Concentration): {ec_mean[i]}")
    print(f"  Slope: {slope_mean[i]}")
    print()

# Step 3: Print control coefficients
print("\n--- Posterior Mean Control Coefficients ---\n")
for i, var in enumerate(control_vars):
    print(f"{var}: {control_coefficients_mean[i]}")

# Step 4: Print the posterior mean for the intercept (base sales impact)
print(f"\nIntercept (Base Sales Impact): {intercept_mean}")

import pickle

# Step 1: Save the model parameters including the scalers and learned parameters
model_params = {
    'beta_media': beta_media_mean,
    'ec': ec_mean,
    'slope': slope_mean,
    'control_coefficients': control_coefficients_mean,
    'intercept': intercept_mean,
    'media_scaler': media_scaler,  # Include the media scaler
    'media_cols': mdsp_cols,
    'control_vars': control_vars,
    'best_alphas': best_alphas  # Include best alphas if you use them
}

# Now save model_params to a file
with open('model_params.pkl', 'wb') as file:
    pickle.dump(model_params, file)

print("Model parameters saved to 'model_params.pkl'")

"""# **Predictions - Base File (Check R2 & MSE)**"""

import numpy as np
import pandas as pd
import pickle

# Step 2: Load the trained model parameters from the .pkl file
with open('model_params.pkl', 'rb') as file:
    model_params = pickle.load(file)

# Extract parameters
beta_media = model_params['beta_media']
ec = model_params['ec']
slope = model_params['slope']
control_coefficients = model_params['control_coefficients']
intercept = model_params['intercept']
media_scaler = model_params['media_scaler']
mdsp_cols = model_params['media_cols']
control_vars = model_params['control_vars']
best_alphas = model_params['best_alphas']

# Step 3: Load the training dataset again (replace 'data.csv' with the actual file)
df_pred = pd.read_csv('data.csv')

df_pred[control_vars].head()

df_pred[mdsp_cols].head() # these are the direct values

df_pred[mdsp_cols] = np.log(df_pred[mdsp_cols] + 1)    # these are log transformed values

df_pred[mdsp_cols].head()

# Step 4: Apply the same scaling as was done during training
df_pred[mdsp_cols] = media_scaler.transform(df_pred[mdsp_cols])

#df_pred[control_vars] = control_scaler.transform(df_pred[control_vars])

df_pred[mdsp_cols].head()  # these are scaled values

# Step 5: Extract media spending (X_media) and control variables (X_controls)
X_media_pred = df_pred[mdsp_cols].values
X_controls_pred = df_pred[control_vars].values

# Step 7: Compute media effects using the learned parameters
def hill_transform(x, ec, slope):
    safe_ec = np.clip(ec, 1e-3, np.inf)  # Ensure ec doesn't go too low
    safe_x = np.clip(x, 1e-3, np.inf)  # Ensure x doesn't go too low
    return 1 / (1 + (safe_x / safe_ec) ** (-slope))

# Step 6: Apply the adstock transformation using the saved best alphas. This will calculate the media spend impact across weeks.
X_media_adstocked_pred = np.array([adstock_transform(X_media_pred[:, i], best_alphas[i]) for i in range(X_media_pred.shape[1])]).T

# Compute media effects
media_effects_pred = np.zeros_like(X_media_adstocked_pred)
for i in range(len(mdsp_cols)):
    media_effects_pred[:, i] = beta_media[i] * hill_transform(X_media_adstocked_pred[:, i], ec[i], slope[i])

# Step 8: Compute control effects
control_effects_pred = np.dot(X_controls_pred, control_coefficients)

# Step 9: Compute predicted log sales (base impact + media effects + control effects)
predicted_sales_log = intercept + np.sum(media_effects_pred, axis=1) + control_effects_pred

# Step 10: Exponentiate to get the predicted sales in original scale
predicted_sales = np.exp(predicted_sales_log) - 1  # Inverse of log transformation

# Step 11: Add predicted sales to the DataFrame
df_pred['predicted_sales'] = predicted_sales

# Step 12: Save the updated DataFrame with predictions
df_pred.to_csv('data_with_predictions.csv', index=False)

print("Predicted sales have been saved to 'data_with_predictions.csv'.")





"""# **Hill Function Outputs for each channel**"""

import numpy as np

# Function to apply hill transformation (diminishing returns)
def hill_transform(x, ec, slope):
    safe_ec = np.clip(ec, 1e-3, np.inf)  # Prevent ec from being too small
    safe_x = np.clip(x, 1e-3, np.inf)    # Prevent media spend from being too small
    return 1 / (1 + (safe_x / safe_ec) ** (-slope))

def calculate_cumulative_sales_impact(media_spend, alpha, beta, ec, slope, n_weeks=15):
    """
    Calculate the cumulative sales impact (immediate + future weeks using adstock).
    """
    # Adstock transformation over n_weeks
    media_spend_accounted = np.zeros(n_weeks)
    media_spend_accounted[0] = media_spend
    for t in range(1, n_weeks):
        media_spend_accounted[t] = alpha * media_spend_accounted[t-1]

    # Immediate and future impacts (log-transformed)
    sales_impact_per_week = [beta * hill_transform(media_spend_accounted[t], ec, slope) for t in range(n_weeks)]

    # Sum of log sales impacts
    cumulative_log_sales_impact = np.sum(sales_impact_per_week)

    # Inverse the log transformation to return to original scale
    cumulative_sales_impact = np.exp(cumulative_log_sales_impact)

    return cumulative_sales_impact  # Return the cumulative impact only

import numpy as np
import matplotlib.pyplot as plt

# Loop through each media channel to generate the Hill curve
fig, axes = plt.subplots(nrows=(len(mdsp_cols) // 3) + 1, ncols=3, figsize=(15, 10))  # Adjust the figure size and layout
axes = axes.ravel()  # Flatten the axes array for easy indexing

for i, media_col in enumerate(mdsp_cols):
    # Get the min and max media spend from the original data (NO LOG TRANSFORM)
    min_spend = df_original[media_col].min()  # Original media spend values
    max_spend = df_original[media_col].max()  # Original media spend values

    # Generate normally distributed media spend values between min and max
    media_spends = np.linspace(min_spend, max_spend, 100)  # No log transformation on spend

    # Define parameters (from the model)
    alpha = best_alphas[i]  # Use the best alpha for the media channel
    beta = beta_media[i]
    ec = ec_mean[i]
    slope = slope_mean[i]

    # Calculate cumulative sales impact for each media spend
    cumulative_impacts = [calculate_cumulative_sales_impact(spend, alpha, beta, ec, slope) for spend in media_spends]

    # Plot the Hill curve on the respective subplot
    axes[i].plot(media_spends, cumulative_impacts, label=f'{media_col}')
    axes[i].axvline(x=df_original[media_col].mean(), color='r', linestyle='--', label='Average Spend')
    axes[i].set_xlabel('Media Spend (Original Scale)')
    axes[i].set_ylabel('Cumulative Sales Impact')
    axes[i].set_title(f'Hill Curve for {media_col}')
    axes[i].legend()
    axes[i].grid(True)

# Hide any empty subplots (if the number of media columns is not a multiple of 3)
for j in range(i+1, len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()





"""# **Part 3**

**Run Predictions and Determine ROAS**
"""

from google.colab import files
uploaded = files.upload()

# Load your new prediction data (from a CSV file)
new_data = pd.read_csv('Prediction_Sheet.csv')

new_data.head()

# Control and media variables
#me_cols = [col for col in df.columns if 'me_' in col] # Macro-economic variables (continuous)
#st_cols = ['st_ct'] # Store count (continuous)
#mrkdn_cols = [col for col in df.columns if 'mrkdn_' in col] # Markdown/Discount (continuous)
#hldy_cols = [col for col in df.columns if 'hldy_' in col] # Holidays (binary)
#seas_cols = [col for col in df.columns if 'seas_' in col] # Seasonality (binary)
seas_cols = ['seas_week_45', 'seas_week_46', 'seas_week_47']

#control_vars_continuous = me_cols + st_cols + mrkdn_cols  # Continuous control variables
control_vars_binary = seas_cols  # Binary control variables
control_vars = control_vars_binary  # All control variables

media_cols = [col for col in df.columns if 'mdsp_' in col]  # Media spending columns

sales_cols = ['sales']

import numpy as np
import pandas as pd
import pickle

# Load the saved parameters and scalers from the .pkl file
with open('model_params.pkl', 'rb') as file:
    model_params = pickle.load(file)

# Extract parameters
beta_media = model_params['beta_media']
ec = model_params['ec']
slope = model_params['slope']
control_coefficients = model_params['control_coefficients']
intercept = model_params['intercept']
media_scaler = model_params['media_scaler']
mdsp_cols = model_params['media_cols']
control_vars = model_params['control_vars']
best_alphas = model_params['best_alphas']  # The adstock decay factors

"""# Control variables from your training logic
#me_cols = [col for col in new_data.columns if 'me_' in col]  # Macro-economic variables (continuous)
#st_cols = ['st_ct']  # Store count (continuous)
#mrkdn_cols = [col for col in new_data.columns if 'mrkdn_' in col]  # Markdown/Discount (continuous)
#hldy_cols = [col for col in new_data.columns if 'hldy_' in col]  # Holidays (binary)
#seas_cols = [col for col in new_data.columns if 'seas_' in col]  # Seasonality (binary)

#control_vars_continuous = me_cols + st_cols + mrkdn_cols  # Continuous control variables
#control_vars_binary = hldy_cols + seas_cols  # Binary control variables


# Step 1: Apply log transformation to continuous variables only (do not log-transform binary variables)
#new_data_log_controls_continuous = np.log(new_data[control_vars_continuous] + 1)  # Log-transform continuous controls

# Combine continuous and binary control variables (binary controls are not log-transformed)
#new_data_log_controls = pd.concat([new_data_log_controls_continuous, new_data[control_vars_binary]], axis=1)

"""
new_data_controls = new_data[control_vars_binary]

# Step 2: Log-transform the media spends
new_data_log_media = np.log(new_data[mdsp_cols] + 1)  # Log-transform the media spends

# Step 3: Now apply MinMaxScaler after log transformation
new_data_scaled_media = media_scaler.transform(new_data_log_media)
#new_data_scaled_controls = control_scaler.transform(new_data_log_controls)

# Adstock transformation function
def adstock_transform(x, alpha):
    """
    Apply adstock transformation to the media spend data.

    Parameters:
    - x: Media spend array (scaled and log-transformed).
    - alpha: Adstock decay factor (specific to each media channel).

    Returns:
    - Adstocked media spend array.
    """
    adstocked = np.zeros_like(x)
    adstocked[0] = x[0]
    for t in range(1, len(x)):
        adstocked[t] = x[t] + alpha * adstocked[t-1]
    return adstocked

# Apply the adstock transformation to the scaled media data using the best alphas
X_media_adstocked_new = np.array([adstock_transform(new_data_scaled_media[:, i], best_alphas[i]) for i in range(len(mdsp_cols))]).T

# Perform predictions using the model parameters like 'beta_media', 'ec', 'slope', etc.
# Example: Predicted sales = intercept + media effect + control effect
# Calculate media effects based on the Hill transformation

def hill_transform(x, ec, slope):
    safe_ec = np.clip(ec, 1e-3, np.inf)  # Prevent ec from being too small
    safe_x = np.clip(x, 1e-3, np.inf)    # Prevent media spend from being too small
    return 1 / (1 + (safe_x / ec) ** (-slope))

# Step 4: Compute media effects using the adstocked media
media_effects_new = np.zeros_like(X_media_adstocked_new)
for i, col in enumerate(mdsp_cols):
    media_effects_new[:, i] = beta_media[i] * hill_transform(X_media_adstocked[:, i], ec[i], slope[i])

# Step 5: Compute control effects using the scaled control variables
control_effects = np.dot(new_data_controls, control_coefficients)

# Step 6: Compute predicted sales (log-transformed)
predicted_sales_log = intercept + np.sum(media_effects, axis=1) + control_effects

# Step 7: Inverse the log transformation to get the predicted sales in original scale
predicted_sales = np.exp(predicted_sales_log) - 1

# Step 8: Add predicted sales to the DataFrame
new_data['predicted_sales'] = predicted_sales

# Step 9: Save the updated DataFrame with predictions
new_data.to_csv('Predictions_with_Sales.csv', index=False)

# Optionally download the file
from google.colab import files
files.download('Predictions_with_Sales.csv')

print("Predicted sales have been calculated and saved to 'Predictions_with_Sales.csv'.")